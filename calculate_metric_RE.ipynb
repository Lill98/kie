{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scp -r infordio-ai@192.168.68.211:/home/infordio-ai/quan/code/information_extraction/PaddleOCR/output/ser/ser_vi_layoutxlm_key_value_20231216_no1_1_no3_1_dataset1 /home/chuongphung/projects/chatgpt/XGBoost/dataset/ser_result\n",
    "# scp -r infordio-ai@192.168.68.211:/home/infordio-ai/quan/code/information_extraction/PaddleOCR/output/ser/ser_vi_layoutxlm_key_value_20231216_no1_1_no3_1_dataset2 /home/chuongphung/projects/chatgpt/XGBoost/dataset/ser_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_txt_ser_data1 = \"/home/chuongphung/projects/chatgpt/XGBoost/dataset/ser_result/ser_vi_layoutxlm_key_value_20231216_no1_1_no3_1_dataset1/infer_results.txt\"\n",
    "root_txt_ser_data2 = \"/home/chuongphung/projects/chatgpt/XGBoost/dataset/ser_result/ser_vi_layoutxlm_key_value_20231216_no1_1_no3_1_dataset2/infer_results.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_re_model = \"20231122_lightgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{root_re_model}/clf.pkl\", 'rb') as f_model:\n",
    "    model_lightgbm = pickle.load(f_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_json_data1 = os.path.join(os.path.dirname(root_txt_ser_data1), \"json_re\")\n",
    "root_json_data2 = os.path.join(os.path.dirname(root_txt_ser_data2), \"json_re\")\n",
    "\n",
    "os.makedirs(root_json_data1, exist_ok=True)\n",
    "os.makedirs(root_json_data2, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(root_txt_ser_data, root_json_data):\n",
    "    with open(root_txt_ser_data, \"r\") as f:\n",
    "        contents = f.read().splitlines()\n",
    "\n",
    "    for content in contents:\n",
    "        # print(content)\n",
    "        name, content_ = content.split(\"\\t\")\n",
    "        name = name.split(\"/\")[-1].replace(\"jpg\",\"json\")\n",
    "        # print(name)\n",
    "        # print(content_)\n",
    "        root_out = os.path.join(root_json_data, name)\n",
    "        content_ = eval(content_)[\"ocr_info\"]\n",
    "        with open(root_out,\"w\", encoding='utf-8') as f:\n",
    "            json.dump(content_, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(root_txt_ser_data1, root_json_data1)\n",
    "convert(root_txt_ser_data2, root_json_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuongphung/.pyenv/versions/3.8.11/envs/information_extraction/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Library\n",
    "import pandas as pd\n",
    "# import lightgbm as lgb\n",
    " \n",
    "# Similarly LGBMRegressor can also be imported for a regression model.\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from box_utils.boxes import *\n",
    "# from data.visualization import Visualization\n",
    "\n",
    "# import glob\n",
    "import pickle\n",
    "# import itertools\n",
    "from tqdm import tqdm\n",
    "from modules.kv_embedding_full_features import KVEmbedding\n",
    "import numpy as np\n",
    "# import time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Preparation_data():\n",
    "    def __init__(self, type=\"label\"):\n",
    "        self.cols = ['k_id', 'k_text', 'k_box', 'v_id', 'v_text', 'v_box', 'k_embed', 'v_embed', 'width', 'height', 'fname']\n",
    "        self.scaler_path = f\"{root_re_model}/scaler.pkl\"\n",
    "        self.scaler = self.load_scaler()\n",
    "        self.device = \"cuda\"\n",
    "        self.kv_embed = KVEmbedding(self.device)\n",
    "        self.type = type\n",
    "    def preprocess_ser2re_batch_ver2(self, im_path, data = None):\n",
    "            h, w = cv2.imread(im_path).shape[:2]\n",
    "            f_name = os.path.basename(im_path)\n",
    "            df_d = pd.DataFrame(data)\n",
    "            # print(df_d)\n",
    "            df_d['width'] = w\n",
    "            df_d['height'] = h\n",
    "            df_d['fname'] = f_name\n",
    "            re = []\n",
    "            # start_time = time.time()\n",
    "            # print(\"--------======\", df_d.transcription.values)\n",
    "            if self.type == \"label\":\n",
    "                df_key = df_d[df_d.label.str.lower()=='question']\n",
    "                if len(df_key) == 0:\n",
    "                    df_key = df_d[df_d.label.str.lower()=='key']\n",
    "            else:\n",
    "                df_key = df_d[df_d.pred.str.lower()=='key']\n",
    "                if len(df_key) == 0:\n",
    "                    df_key = df_d[df_d.pred.str.lower()=='question']\n",
    "            if df_key.shape[0] == 0:\n",
    "                # print(\"No question-answer pair was found\")\n",
    "                re = pd.DataFrame(re)\n",
    "                return re\n",
    "            df_key_transcription = df_key.transcription.values.tolist()\n",
    "            df_key_transcription = self.kv_embed.embedding(df_key_transcription)\n",
    "            df_key[\"embedding\"] = df_key_transcription.tolist()\n",
    "            \n",
    "            if self.type == \"label\":\n",
    "                df_value = df_d[df_d.label.str.lower()=='answer']\n",
    "                if len(df_value) == 0:\n",
    "                    df_value = df_d[df_d.label.str.lower()=='value']\n",
    "            else:\n",
    "                df_value = df_d[df_d.pred.str.lower()=='value']\n",
    "                if len(df_value) == 0:\n",
    "                    df_value = df_d[df_d.pred.str.lower()=='answer']\n",
    "            if df_value.shape[0] == 0:\n",
    "                # print(\"No question-answer pair was found\")\n",
    "                re = pd.DataFrame(re)\n",
    "                return re\n",
    "            df_value_transcription = df_value.transcription.values.tolist()\n",
    "            df_value_transcription = self.kv_embed.embedding(df_value_transcription)\n",
    "            df_value[\"embedding\"] = df_value_transcription.tolist()\n",
    "            \n",
    "            for key in df_key.iterrows():\n",
    "                linking = key[-1].linking\n",
    "                for value in df_value.iterrows():\n",
    "                    if [key[-1].id, value[-1].id] in linking:\n",
    "                        link_label =1.0\n",
    "                    else:\n",
    "                        link_label =0.0\n",
    "                    re.append({\n",
    "                        'k_id': key[-1].id,\n",
    "                        'k_text': str(key[-1].transcription),\n",
    "                        'k_embed': key[-1].embedding,\n",
    "                        'k_box': points2xyxy(key[-1].points),\n",
    "                        'v_id': value[-1].id,\n",
    "                        'v_text': value[-1].transcription,\n",
    "                        'v_embed': value[-1].embedding,\n",
    "                        'v_box': points2xyxy(value[-1].points),\n",
    "                        'width': w,\n",
    "                        'height': h,\n",
    "                        'fname': os.path.basename(f_name),\n",
    "                        'label': link_label\n",
    "                    })\n",
    "            # print(\"------------================time 2 for loops\", time.time()-start_time)\n",
    "            re = pd.DataFrame(re)\n",
    "            if re.shape[0] == 0:\n",
    "                # print(\"No question-answer pair was found\")\n",
    "                return re\n",
    "        \n",
    "            return re.reset_index(drop=True)\n",
    "\n",
    "    def make_features(self, df:pd.DataFrame):\n",
    "        \"\"\"Create feature from dataframe\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): input data\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: feature after process\n",
    "        \"\"\"\n",
    "        # print(self.cols + ['label'])\n",
    "        df = df[self.cols + ['label']]\n",
    "        df.k_box = df.apply(lambda x: normalize_scale_bbox(x.k_box, x.width, x.height), axis=1)\n",
    "        df.v_box = df.apply(lambda x:normalize_scale_bbox(x.v_box, x.width, x.height), axis=1)\n",
    "        k_features = pd.DataFrame(df.k_box.tolist(), index=df.index, columns=['k_' + s for s in ['x1', 'y1', 'x2', 'y2']])\n",
    "        v_features = pd.DataFrame(df.v_box.tolist(), index=df.index, columns=['v_' + s for s in ['x1', 'y1', 'x2', 'y2']])\n",
    "        \n",
    "        df = pd.concat([k_features, v_features, df[self.cols], df['label']], axis=1)\n",
    "        \n",
    "        df['k_cx'] = df.k_x1.add(df.k_x2).div(2)\n",
    "        df['k_cy'] = df.k_y1.add(df.k_y2).div(2)\n",
    "        \n",
    "        df['v_cx'] = df.v_x1.add(df.v_x2).div(2)\n",
    "        df['v_cy'] = df.v_y1.add(df.v_y2).div(2)\n",
    "        \n",
    "        df['fe1'] = abs(df.v_x1 - df.k_x1)\n",
    "        df['fe2'] = abs(df.v_y1 - df.k_y1)\n",
    "        df['fe3'] = abs(df.v_x1 - df.k_x2)\n",
    "        df['fe4'] = abs(df.v_y1 - df.k_y2)\n",
    "        df['fe5'] = abs(df.v_x2 - df.k_x1)\n",
    "        df['fe6'] = abs(df.v_y2 - df.k_y1)\n",
    "        df['fe7'] = abs(df.v_x2 - df.k_x2)\n",
    "        df['fe8'] = abs(df.v_y2 - df.k_y2)\n",
    "        df['fe9'] = abs(df.v_x2 - df.v_x1)\n",
    "        df['fe10'] = abs(df.v_y2 - df.v_y1)\n",
    "        df['fe11'] = abs(df.k_x2 - df.k_x1)\n",
    "        df['fe12'] = abs(df.k_y2 - df.k_y1)\n",
    "        \n",
    "        df['fe13'] = df.apply(lambda x: cal_degrees([x.k_x1, x.k_y1], [x.v_x1, x.v_y1]), axis=1)\n",
    "        df['fe14'] = df.apply(lambda x: cal_degrees([x.k_x2, x.k_y1], [x.v_x2, x.v_y1]), axis=1)\n",
    "        df['fe15'] = df.apply(lambda x: cal_degrees([x.k_x2, x.k_y2], [x.v_x2, x.v_y2]), axis=1)\n",
    "        df['fe16'] = df.apply(lambda x: cal_degrees([x.k_x1, x.k_y2], [x.v_x1, x.v_y2]), axis=1)\n",
    "        df['fe17'] = df.apply(lambda x: cal_degrees([x['k_cx'], x['k_cy']], [x['v_cx'], x['v_cy']]), axis=1)\n",
    "        \n",
    "        df['fe18'] = df.apply(lambda x: boxes_distance([x.k_x1-x.v_x2, x.k_y2-x.v_y1],[x.v_x1-x.k_x2, x.v_y2-x.k_y1]), axis=1)\n",
    "        df['fe19'] = df.apply(lambda x: dist_points([x.k_cx, x.k_cy], [x.v_cx, x.v_cy]), axis=1)\n",
    "        \n",
    "        k_embed_df = pd.DataFrame(np.array(df['k_embed'].values.tolist())).add_prefix('fe20')\n",
    "        df = pd.concat([df, k_embed_df], axis=1)\n",
    "        \n",
    "        v_embed_df = pd.DataFrame(np.array(df['k_embed'].values.tolist())).add_prefix('fe21')\n",
    "        df = pd.concat([df, v_embed_df], axis=1)\n",
    "        \n",
    "        cols = [c for c in df.columns if c.startswith('fe')] + ['label']\n",
    "\n",
    "        return df[cols], df[self.cols]\n",
    "    \n",
    "    def load_scaler(self):\n",
    "        # print('Loading scaler post processing relation ...')\n",
    "        if os.path.exists(self.scaler_path):\n",
    "            with open(self.scaler_path, 'rb') as f_scaler:\n",
    "                scaler = pickle.load(f_scaler)\n",
    "            f_scaler.close()\n",
    "            return scaler \n",
    "        else:\n",
    "            print(\"Path to scaler not exist !\")\n",
    "    \n",
    "    def run(self, im_path, data):\n",
    "        data = self.preprocess_ser2re_batch_ver2(im_path, data)\n",
    "        if len(data)==0:\n",
    "            return [], [], []\n",
    "        d_features, __ = self.make_features(data)\n",
    "        X, y = d_features.values[:, :-1], d_features.values[:, -1]\n",
    "        X_transform = self.scaler.transform(X)\n",
    "        return X_transform, y, data\n",
    "# with open(\"/home/chuongphung/projects/chatgpt/XGBoost/dataset/20231030_re/train/train_original.json\") as f:\n",
    "#     data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_eval_ser(path_json_ser):\n",
    "    path_image = os.path.dirname(path_json_ser)\n",
    "    preparation_engine = Preparation_data(\"ser\")\n",
    "    \n",
    "    X_val = np.array([])\n",
    "    Y_val = np.array([])\n",
    "    DF_val = []\n",
    "    for doc_name in tqdm(os.listdir(path_json_ser)):\n",
    "        if \".json\" not in doc_name: continue\n",
    "        with open(os.path.join(path_json_ser, doc_name)) as f:\n",
    "            doc = json.load(f)\n",
    "        im_path = os.path.join(path_image, doc_name.replace(\".json\", \"_ser.jpg\"))\n",
    "        x_val, y_val, df_val = preparation_engine.run(im_path, doc)\n",
    "        if len(y_val)!=0:\n",
    "            if X_val.shape[0] == 0:\n",
    "                X_val = x_val\n",
    "            else:\n",
    "                X_val = np.vstack((X_val, x_val))\n",
    "            \n",
    "            if Y_val.shape[0] == 0:\n",
    "                Y_val = y_val\n",
    "            else:\n",
    "                Y_val = np.hstack((Y_val, y_val))\n",
    "            if len(DF_val)==0:\n",
    "                DF_val = df_val\n",
    "            else:\n",
    "                DF_val = pd.concat([DF_val, df_val], ignore_index=True,axis=0)\n",
    "        # break\n",
    "    print(X_val.shape)\n",
    "    print(Y_val.shape)\n",
    "    return X_val, Y_val, DF_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_eval_label(path_json_ser):\n",
    "    path_image = os.path.dirname(path_json_ser)\n",
    "    \n",
    "    preparation_engine = Preparation_data(\"label\")\n",
    "    \n",
    "    X_val = np.array([])\n",
    "    Y_val = np.array([])\n",
    "    DF_val = []\n",
    "    for doc_name in tqdm(os.listdir(path_json_ser)):\n",
    "        if \".json\" not in doc_name: continue\n",
    "        with open(os.path.join(path_json_ser, doc_name)) as f:\n",
    "            doc = json.load(f)\n",
    "        im_path = os.path.join(path_image,doc_name.replace(\".json\", \"_ser.jpg\"))\n",
    "        x_val, y_val, df_val = preparation_engine.run(im_path, doc)\n",
    "        if len(y_val)!=0:\n",
    "            if X_val.shape[0] == 0:\n",
    "                X_val = x_val\n",
    "            else:\n",
    "                X_val = np.vstack((X_val, x_val))\n",
    "            \n",
    "            if Y_val.shape[0] == 0:\n",
    "                Y_val = y_val\n",
    "            else:\n",
    "                Y_val = np.hstack((Y_val, y_val))\n",
    "            if len(DF_val)==0:\n",
    "                DF_val = df_val\n",
    "            else:\n",
    "                DF_val = pd.concat([DF_val, df_val], ignore_index=True,axis=0)\n",
    "        # break\n",
    "    print(X_val.shape)\n",
    "    print(Y_val.shape)\n",
    "    return X_val, Y_val, DF_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(df: pd.DataFrame, threshold = 0.25):\n",
    "    # one value only links to one key but one key can link to many value\n",
    "    # df['pred_prob'] = pred_prob\n",
    "    df['is_linking'] = 0\n",
    "    fnames = df.fname.unique().tolist()\n",
    "    for fname in fnames:\n",
    "        df_fname = df[df.fname==fname]\n",
    "        v_ids = df_fname.v_id.unique().tolist()\n",
    "        for v_id in v_ids:\n",
    "            df_vid = df_fname[df_fname.v_id==v_id]\n",
    "            idx_max = df_vid.pred_prob.idxmax()\n",
    "\n",
    "            if df.loc[(df.fname==fname)&(df.v_id==v_id)&(df.index==idx_max), 'pred_prob'].values[0] >= threshold:\n",
    "                df.loc[(df.fname==fname)&(df.v_id==v_id)&(df.index==idx_max), 'is_linking'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(DF_val_ser, Y_val_ser, DF_val_label, threadhold=0):\n",
    "    new_df_1 = post_process(DF_val_ser.copy(), threadhold)\n",
    "    tn, fp, fn, tp =  confusion_matrix(Y_val_ser, new_df_1[\"is_linking\"]).ravel()\n",
    "    print(tn, fp, fn, tp)\n",
    "    number_positive = len(DF_val_label[DF_val_label[\"label\"]==1])\n",
    "    recall = tp/number_positive\n",
    "    precision = tp/(tp+fp)\n",
    "    f1 = 2/((recall+precision)/(precision*recall))\n",
    "    print(\"recall\", recall)\n",
    "    print(\"precision\", precision)\n",
    "    print(\"f1\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:21<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12905, 787)\n",
      "(12905,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:21<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13258, 787)\n",
      "(13258,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_val_ser1, Y_val_ser1, DF_val_ser1 = prepare_data_eval_ser(root_json_data1)\n",
    "X_val_label1, Y_val_label1, DF_val_label1 = prepare_data_eval_label(root_json_data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lightgbm = model_lightgbm.predict(X_val_ser1)\n",
    "DF_val_ser1[\"pred_prob\"] = model_lightgbm.predict_proba(X_val_ser1)[:, 1].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11435 210 26 1234\n",
      "recall 0.8371777476255088\n",
      "precision 0.8545706371191135\n",
      "f1 0.8457847840986976\n"
     ]
    }
   ],
   "source": [
    "# thread 0\n",
    "calculate_metric(DF_val_ser1, Y_val_ser1, DF_val_label1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11517 128 85 1175\n",
      "recall 0.7971506105834464\n",
      "precision 0.9017651573292402\n",
      "f1 0.8462369463449766\n"
     ]
    }
   ],
   "source": [
    "# thread 0.25\n",
    "calculate_metric(DF_val_ser1, Y_val_ser1, DF_val_label1, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [01:13<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51030, 787)\n",
      "(51030,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [01:24<00:00,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67685, 787)\n",
      "(67685,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_val_ser2, Y_val_ser2, DF_val_ser2 = prepare_data_eval_ser(root_json_data2)\n",
    "X_val_label2, Y_val_label2, DF_val_label2 = prepare_data_eval_label(root_json_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lightgbm = model_lightgbm.predict(X_val_ser2)\n",
    "DF_val_ser2[\"pred_prob\"] = model_lightgbm.predict_proba(X_val_ser2)[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46661 1352 176 2841\n",
      "recall 0.6827685652487383\n",
      "precision 0.6775578344860482\n",
      "f1 0.6801532200143644\n"
     ]
    }
   ],
   "source": [
    "# thread 0\n",
    "calculate_metric(DF_val_ser2, Y_val_ser2, DF_val_label2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47285 728 441 2576\n",
      "recall 0.6190819514539774\n",
      "precision 0.7796610169491526\n",
      "f1 0.6901540522438043\n"
     ]
    }
   ],
   "source": [
    "# thread 0.25\n",
    "calculate_metric(DF_val_ser2, Y_val_ser2, DF_val_label2, 0.25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "information_extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
