{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize - run without asking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuongphung/.pyenv/versions/3.8.11/envs/information_extraction/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Library\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    " \n",
    "# Similarly LGBMRegressor can also be imported for a regression model.\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from box_utils.boxes import *\n",
    "from data.visualization import Visualization\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from modules.kv_embedding_full_features import KVEmbedding\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class Preparation_data():\n",
    "    def __init__(self):\n",
    "        self.cols = ['k_id', 'k_text', 'k_box', 'v_id', 'v_text', 'v_box', 'k_embed', 'v_embed', 'width', 'height', 'fname']\n",
    "        # self.scaler_path = \"/home/chuongphung/projects/chatgpt/XGBoost/20231106_xgboost_weights/scaler.pkl\"\n",
    "        # self.scaler = self.load_scaler()\n",
    "        self.device = \"cuda\"\n",
    "        self.kv_embed = KVEmbedding(self.device)\n",
    "        \n",
    "    def preprocess_ser2re_batch_ver2(self, im_path, data = None):\n",
    "            h, w = cv2.imread(im_path).shape[:2]\n",
    "            f_name = os.path.basename(im_path)\n",
    "            df_d = pd.DataFrame(data)\n",
    "            df_d['width'] = w\n",
    "            df_d['height'] = h\n",
    "            df_d['fname'] = f_name\n",
    "            re = []\n",
    "            # start_time = time.time()\n",
    "            # print(\"--------======\", df_d.transcription.values)\n",
    "            df_key = df_d[df_d.label.str.lower()=='question']\n",
    "            if df_key.shape[0] == 0:\n",
    "                # print(\"No question-answer pair was found\")\n",
    "                re = pd.DataFrame(re)\n",
    "                return re\n",
    "            df_key_transcription = df_key.transcription.values.tolist()\n",
    "            df_key_transcription = self.kv_embed.embedding(df_key_transcription)\n",
    "            df_key[\"embedding\"] = df_key_transcription.tolist()\n",
    "            \n",
    "            df_value = df_d[df_d.label.str.lower()=='answer']\n",
    "            if df_value.shape[0] == 0:\n",
    "                # print(\"No question-answer pair was found\")\n",
    "                re = pd.DataFrame(re)\n",
    "                return re\n",
    "            df_value_transcription = df_value.transcription.values.tolist()\n",
    "            df_value_transcription = self.kv_embed.embedding(df_value_transcription)\n",
    "            df_value[\"embedding\"] = df_value_transcription.tolist()\n",
    "            \n",
    "            for key in df_key.iterrows():\n",
    "                linking = key[-1].linking\n",
    "                for value in df_value.iterrows():\n",
    "                    if [key[-1].id, value[-1].id] in linking:\n",
    "                        link_label =1.0\n",
    "                    else:\n",
    "                        link_label =0.0\n",
    "                    re.append({\n",
    "                        'k_id': key[-1].id,\n",
    "                        'k_text': str(key[-1].transcription),\n",
    "                        'k_embed': key[-1].embedding,\n",
    "                        'k_box': points2xyxy(key[-1].points),\n",
    "                        'v_id': value[-1].id,\n",
    "                        'v_text': value[-1].transcription,\n",
    "                        'v_embed': value[-1].embedding,\n",
    "                        'v_box': points2xyxy(value[-1].points),\n",
    "                        'width': w,\n",
    "                        'height': h,\n",
    "                        'fname': os.path.basename(f_name),\n",
    "                        'label': link_label\n",
    "                    })\n",
    "            # print(\"------------================time 2 for loops\", time.time()-start_time)\n",
    "            re = pd.DataFrame(re)\n",
    "            if re.shape[0] == 0:\n",
    "                # print(\"No question-answer pair was found\")\n",
    "                return re\n",
    "        \n",
    "            return re.reset_index(drop=True)\n",
    "\n",
    "    def make_features(self, df:pd.DataFrame):\n",
    "        \"\"\"Create feature from dataframe\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): input data\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: feature after process\n",
    "        \"\"\"\n",
    "        # print(self.cols + ['label'])\n",
    "        df = df[self.cols + ['label']]\n",
    "        df.k_box = df.apply(lambda x: normalize_scale_bbox(x.k_box, x.width, x.height), axis=1)\n",
    "        df.v_box = df.apply(lambda x:normalize_scale_bbox(x.v_box, x.width, x.height), axis=1)\n",
    "        k_features = pd.DataFrame(df.k_box.tolist(), index=df.index, columns=['k_' + s for s in ['x1', 'y1', 'x2', 'y2']])\n",
    "        v_features = pd.DataFrame(df.v_box.tolist(), index=df.index, columns=['v_' + s for s in ['x1', 'y1', 'x2', 'y2']])\n",
    "        \n",
    "        df = pd.concat([k_features, v_features, df[self.cols], df['label']], axis=1)\n",
    "        \n",
    "        df['k_cx'] = df.k_x1.add(df.k_x2).div(2)\n",
    "        df['k_cy'] = df.k_y1.add(df.k_y2).div(2)\n",
    "        \n",
    "        df['v_cx'] = df.v_x1.add(df.v_x2).div(2)\n",
    "        df['v_cy'] = df.v_y1.add(df.v_y2).div(2)\n",
    "        \n",
    "        df['fe1'] = abs(df.v_x1 - df.k_x1)\n",
    "        df['fe2'] = abs(df.v_y1 - df.k_y1)\n",
    "        df['fe3'] = abs(df.v_x1 - df.k_x2)\n",
    "        df['fe4'] = abs(df.v_y1 - df.k_y2)\n",
    "        df['fe5'] = abs(df.v_x2 - df.k_x1)\n",
    "        df['fe6'] = abs(df.v_y2 - df.k_y1)\n",
    "        df['fe7'] = abs(df.v_x2 - df.k_x2)\n",
    "        df['fe8'] = abs(df.v_y2 - df.k_y2)\n",
    "        df['fe9'] = abs(df.v_x2 - df.v_x1)\n",
    "        df['fe10'] = abs(df.v_y2 - df.v_y1)\n",
    "        df['fe11'] = abs(df.k_x2 - df.k_x1)\n",
    "        df['fe12'] = abs(df.k_y2 - df.k_y1)\n",
    "        \n",
    "        df['fe13'] = df.apply(lambda x: cal_degrees([x.k_x1, x.k_y1], [x.v_x1, x.v_y1]), axis=1)\n",
    "        df['fe14'] = df.apply(lambda x: cal_degrees([x.k_x2, x.k_y1], [x.v_x2, x.v_y1]), axis=1)\n",
    "        df['fe15'] = df.apply(lambda x: cal_degrees([x.k_x2, x.k_y2], [x.v_x2, x.v_y2]), axis=1)\n",
    "        df['fe16'] = df.apply(lambda x: cal_degrees([x.k_x1, x.k_y2], [x.v_x1, x.v_y2]), axis=1)\n",
    "        df['fe17'] = df.apply(lambda x: cal_degrees([x['k_cx'], x['k_cy']], [x['v_cx'], x['v_cy']]), axis=1)\n",
    "        \n",
    "        df['fe18'] = df.apply(lambda x: boxes_distance([x.k_x1-x.v_x2, x.k_y2-x.v_y1],[x.v_x1-x.k_x2, x.v_y2-x.k_y1]), axis=1)\n",
    "        df['fe19'] = df.apply(lambda x: dist_points([x.k_cx, x.k_cy], [x.v_cx, x.v_cy]), axis=1)\n",
    "        \n",
    "        # print(\"============//////////////============\", np.array(df['k_embed'].values.tolist()))\n",
    "        k_embed_df = pd.DataFrame(np.array(df['k_embed'].values.tolist())).add_prefix('fe20')\n",
    "        # print(\"========================\", k_embed_df.shape)\n",
    "        df = pd.concat([df, k_embed_df], axis=1)\n",
    "        v_embed_df = pd.DataFrame(np.array(df['k_embed'].values.tolist())).add_prefix('fe21')\n",
    "        # print(\"========================\", v_embed_df.shape)\n",
    "        df = pd.concat([df, v_embed_df], axis=1)\n",
    "        # print(\"========================\", df.shape)\n",
    "        cols = [c for c in df.columns if c.startswith('fe')] + ['label']\n",
    "\n",
    "        return df[cols], df[self.cols]\n",
    "    \n",
    "    # def load_scaler(self):\n",
    "    #     # print('Loading scaler post processing relation ...')\n",
    "    #     if os.path.exists(self.scaler_path):\n",
    "    #         with open(self.scaler_path, 'rb') as f_scaler:\n",
    "    #             scaler = pickle.load(f_scaler)\n",
    "    #         f_scaler.close()\n",
    "    #         return scaler \n",
    "    #     else:\n",
    "    #         print(\"Path to scaler not exist !\")\n",
    "    \n",
    "    def run(self, im_path, data):\n",
    "        data = self.preprocess_ser2re_batch_ver2(im_path, data)\n",
    "        if len(data)==0:\n",
    "            return [], [], []\n",
    "        d_features, __ = self.make_features(data)\n",
    "        \n",
    "        X, y = d_features.values[:, :-1], d_features.values[:, -1]\n",
    "        # print(\"===========X.shape=============\", X.shape)\n",
    "        # X_transform = self.scaler.transform(X)\n",
    "        return X, y, data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_engine = Preparation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training- eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_dot_json = \"/home/chuongphung/projects/chatgpt/XGBoost/dataset/no1_1_no3_1/final_qa_same_20230927/train/train.json\"\n",
    "path_train_image = \"/home/chuongphung/projects/chatgpt/XGBoost/dataset/no1_1_no3_1/final_qa_same_20230927/train/image/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------================Len train dataset:  1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:16<00:00,  6.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10961, 787)\n",
      "(10961,)\n",
      "[1. 0. 0. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path_train_dot_json, \"rb\") as f:\n",
    "    infer_imgs = f.readlines()\n",
    "print(\"-------------================Len train dataset: \", len(infer_imgs))\n",
    "X_train = np.array([])\n",
    "Y_train = np.array([])\n",
    "for doc in tqdm(infer_imgs[:]):\n",
    "    data_line = doc.decode('utf-8')\n",
    "    substr = data_line.strip(\"\\n\").split(\"\\t\")\n",
    "    im_path = os.path.join(path_train_image, substr[0])\n",
    "    try:\n",
    "        x_train, y_train, _ = preparation_engine.run(im_path, eval(substr[1]))\n",
    "    except:\n",
    "        print(substr[0])\n",
    "        continue\n",
    "    if len(y_train)!=0:\n",
    "        if X_train.shape[0] == 0:\n",
    "            X_train = x_train\n",
    "        else:\n",
    "            X_train = np.vstack((X_train, x_train))\n",
    "        \n",
    "        if Y_train.shape[0] == 0:\n",
    "            Y_train = y_train\n",
    "        else:\n",
    "            Y_train = np.hstack((Y_train, y_train))\n",
    "    # break\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_eval_dot_json = \"/home/chuongphung/projects/chatgpt/XGBoost/dataset/test_dataset/dataset2/val/val.json\"\n",
    "path_eval_image = \"/home/chuongphung/projects/chatgpt/XGBoost/dataset/test_dataset/dataset2/val/image/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------================Len train dataset:  517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 787)\n",
      "(802,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path_eval_dot_json, \"rb\") as f:\n",
    "    infer_imgs = f.readlines()\n",
    "print(\"-------------================Len train dataset: \", len(infer_imgs))\n",
    "X_val = np.array([])\n",
    "Y_val = np.array([])\n",
    "DF_val = []\n",
    "for doc in tqdm(infer_imgs[:]):\n",
    "    data_line = doc.decode('utf-8')\n",
    "    substr = data_line.strip(\"\\n\").split(\"\\t\")\n",
    "    im_path = os.path.join(path_eval_image, substr[0])\n",
    "    \n",
    "    x_val, y_val, df_val = preparation_engine.run(im_path, eval(substr[1]))\n",
    "    if len(y_val)!=0:\n",
    "        if X_val.shape[0] == 0:\n",
    "            X_val = x_val\n",
    "        else:\n",
    "            X_val = np.vstack((X_val, x_val))\n",
    "        \n",
    "        if Y_val.shape[0] == 0:\n",
    "            Y_val = y_val\n",
    "        else:\n",
    "            Y_val = np.hstack((Y_val, y_val))\n",
    "        if len(DF_val)==0:\n",
    "            DF_val = df_val\n",
    "        else:\n",
    "            DF_val = pd.concat([DF_val, df_val], ignore_index=True,axis=0)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = \"20231220_lightgbm/\"\n",
    "os.makedirs(SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "pickle.dump(scaler, open(os.path.join(SAVE_PATH, \"scaler.pkl\"), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1128, number of negative: 9833\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036767 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 196396\n",
      "[LightGBM] [Info] Number of data points in the train set: 10961, number of used features: 787\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "----------================Eval ...\n",
      "Training accuracy 1.0000\n",
      "Testing accuracy 0.9763\n",
      "----------================Saving model ...\n",
      "-----------===========precision_score [0.97858099 0.94545455]\n",
      "-----------===========recall_score [0.99591281 0.76470588]\n",
      "-----------===========f1_score [0.98717083 0.84552846]\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "            'random_state': 1997,\n",
    "            'n_estimators': 200,\n",
    "            'n_jobs': 15,\n",
    "            'max_depth': 70,\n",
    "            'num_leaves': 100,\n",
    "            'class_weight': 'balanced' #{0: 0.31, 1:0.69}\n",
    "        }\n",
    "model = LGBMClassifier(objective=\"binary\", **params)\n",
    "model.fit(X_train, Y_train)\n",
    "print(\"----------================Eval ...\")\n",
    "print('Training accuracy {:.4f}'.format(model.score(X_train,Y_train)))\n",
    "print('Testing accuracy {:.4f}'.format(model.score(X_val,Y_val)))\n",
    "print('----------================Saving model ...')\n",
    "with open(os.path.join(SAVE_PATH, 'clf.pkl'), 'wb') as f_cls:\n",
    "    pickle.dump(model, f_cls, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# Predicting the Target variable\n",
    "pred = model.predict(X_val)\n",
    "print(\"-----------===========precision_score\", precision_score(Y_val, pred, average=None))\n",
    "print(\"-----------===========recall_score\", recall_score(Y_val, pred, average=None))\n",
    "print(\"-----------===========f1_score\", f1_score(Y_val, pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_PATH, 'clf.pkl'), 'rb') as f_model:\n",
    "    model_lightgbm = pickle.load(f_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval with post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_val[\"pred_prob\"] = model_lightgbm.predict_proba(X_val)[:, 1].tolist()\n",
    "DF_val[\"pred\"] = model_lightgbm.predict(X_val).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(df: pd.DataFrame, threshold = 0.25):\n",
    "    # one value only links to one key but one key can link to many value\n",
    "    # df['pred_prob'] = pred_prob\n",
    "    df['is_linking'] = 0\n",
    "    fnames = df.fname.unique().tolist()\n",
    "    for fname in fnames:\n",
    "        df_fname = df[df.fname==fname]\n",
    "        v_ids = df_fname.v_id.unique().tolist()\n",
    "        for v_id in v_ids:\n",
    "            df_vid = df_fname[df_fname.v_id==v_id]\n",
    "            idx_max = df_vid.pred_prob.idxmax()\n",
    "\n",
    "            if df.loc[(df.fname==fname)&(df.v_id==v_id)&(df.index==idx_max), 'pred_prob'].values[0] >= threshold:\n",
    "                df.loc[(df.fname==fname)&(df.v_id==v_id)&(df.index==idx_max), 'is_linking'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------===========precision_score [0.97858099 0.94545455]\n",
      "-----------===========recall_score [0.99591281 0.76470588]\n",
      "-----------===========f1_score [0.98717083 0.84552846]\n"
     ]
    }
   ],
   "source": [
    "# Without post-processing\n",
    "print(\"-----------===========precision_score\", precision_score(DF_val[\"label\"].values, DF_val[\"pred\"].values, average=None))\n",
    "print(\"-----------===========recall_score\", recall_score(DF_val[\"label\"].values, DF_val[\"pred\"].values, average=None))\n",
    "print(\"-----------===========f1_score\", f1_score(DF_val[\"label\"].values, DF_val[\"pred\"].values, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------===========precision_score [0.9972752  0.97058824]\n",
      "-----------===========recall_score [0.9972752  0.97058824]\n",
      "-----------===========f1_score [0.9972752  0.97058824]\n"
     ]
    }
   ],
   "source": [
    "# With post-processing threshold = 0.0\n",
    "new_df = post_process(DF_val.copy(), 0)\n",
    "print(\"-----------===========precision_score\", precision_score(new_df[\"label\"].values, new_df[\"is_linking\"].values, average=None))\n",
    "print(\"-----------===========recall_score\", recall_score(new_df[\"label\"].values, new_df[\"is_linking\"].values, average=None))\n",
    "print(\"-----------===========f1_score\", f1_score(new_df[\"label\"].values, new_df[\"is_linking\"].values, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------===========precision_score [0.97991968 0.96363636]\n",
      "-----------===========recall_score [0.9972752  0.77941176]\n",
      "-----------===========f1_score [0.98852127 0.86178862]\n"
     ]
    }
   ],
   "source": [
    "# With post-processing threshold = 0.25\n",
    "new_df = post_process(DF_val.copy(), 0.25)\n",
    "print(\"-----------===========precision_score\", precision_score(new_df[\"label\"].values, new_df[\"is_linking\"].values, average=None))\n",
    "print(\"-----------===========recall_score\", recall_score(new_df[\"label\"].values, new_df[\"is_linking\"].values, average=None))\n",
    "print(\"-----------===========f1_score\", f1_score(new_df[\"label\"].values, new_df[\"is_linking\"].values, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('information_extraction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a080eea0d32afbf655ae670c0c5d9e130e29d6ce8f8be4e0db579b60be473365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
